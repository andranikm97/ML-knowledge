{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8613b80e",
   "metadata": {},
   "source": [
    "# MNIST neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31590f1f",
   "metadata": {},
   "source": [
    "Only using packages I truly don't want to replicate, like pandas, numpy, tf and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08788749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45e186",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist_data\n",
    "\n",
    "train_X, val_X, train_y, val_y =  train_test_split(train_X, train_y, test_size=0.25)\n",
    "\n",
    "train_X = train_X.reshape(train_X.shape[0], -1) / 255.\n",
    "train_y = tf.one_hot(train_y, depth=10)\n",
    "\n",
    "val_X = val_X.reshape(val_X.shape[0], -1) / 255.\n",
    "val_y = tf.one_hot(val_y, depth=10)\n",
    "\n",
    "test_X = test_X.reshape(test_X.shape[0], -1) / 255.\n",
    "test_y = tf.one_hot(test_y, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training features shape: ', train_X.shape)\n",
    "print('Training labels shape: ', train_y.shape)\n",
    "\n",
    "print('Validation features shape: ', val_X.shape)\n",
    "print('Validation labels shape: ', val_y.shape)\n",
    "\n",
    "print('Testing examples shape: ', test_X.shape)\n",
    "print('Testin labels shape: ', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429af431",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActivationDerivative(activation):\n",
    "    if activation == 'linear':\n",
    "        return lambda x : np.ones(x.shape)\n",
    "\n",
    "    if activation == 'relu':\n",
    "        return lambda x : x > 0\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        g = tf.keras.activations.sigmoid\n",
    "        return lambda x : g(x) * (1 - g(x))\n",
    "    \n",
    "def printProgressBar(progress):\n",
    "    maxLen = 40\n",
    "    markerIncomplete = \"-\"\n",
    "    markerComplete = \"=\"\n",
    "    currentIndex = math.ceil(progress * maxLen)\n",
    "    return markerComplete * currentIndex + \">\" + markerIncomplete * (maxLen - currentIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3467972",
   "metadata": {},
   "source": [
    "## Write _Dense_ and _Sequential_ (mimic TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, units, activation=None):\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.activationDerivative = getActivationDerivative(activation)\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.Z = None\n",
    "        self.A_in = None\n",
    "        self.A_out = None\n",
    "        self.built = False\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = np.random.randn(self.units, input_shape)\n",
    "        self.b = np.random.randn(self.units, 1)\n",
    "        self.built = True\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return (self.W, self.b)\n",
    "    \n",
    "    def call(self, A, dry, verbose=False):\n",
    "        if self.built == False:\n",
    "            raise Error(\"Layer has not been built yet\")\n",
    "        if verbose:\n",
    "            print()\n",
    "            print('units', self.units)\n",
    "            print('A_in:', A.shape)\n",
    "            print('W:', self.W.shape)\n",
    "            print('B:', self.b.shape)\n",
    "            print('A_out:', (np.dot(self.W, A) + self.b).shape)\n",
    "            print()\n",
    "        \n",
    "        Z = np.dot(self.W, A) + self.b\n",
    "        A_out = self.activation(Z)\n",
    "        \n",
    "        if not(dry):\n",
    "            self.A_in = A\n",
    "            self.Z = Z\n",
    "            self.A_out = A_out\n",
    "        return A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.input_shape = None;\n",
    "        self.layers = layers\n",
    "        self.size = len(layers)\n",
    "        self.output_layer = layers[-1]\n",
    "        self.built = False;\n",
    "        self.errorHist = [];\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        for layer in self.layers:\n",
    "            layer.build(input_shape)\n",
    "            input_shape = layer.units\n",
    "        self.built = True\n",
    "     \n",
    "    def forwardProp(self, activation, dry=False):\n",
    "        for layer in self.layers:\n",
    "            activation = layer.call(activation, dry)\n",
    "        return activation\n",
    "        \n",
    "    def backwardProp(self, dL, learningScalar, DW, DB):\n",
    "        dl = dL\n",
    "        for i in range(self.size - 2, -1, -1):\n",
    "            next_layer = self.layers[i+1]\n",
    "            current_layer = self.layers[i]\n",
    "            Z_l = current_layer.Z\n",
    "            dl = np.dot(np.transpose(next_layer.W), dl) * current_layer.activationDerivative(Z_l)\n",
    "            DW[i] = learningScalar * np.dot(dl, np.transpose(current_layer.A_in))\n",
    "            DB[i] = learningScalar * dl\n",
    "\n",
    "    def fit(self, X, y, val_X=None, val_y=None,  epochs=1, eta=0.001, batchSize=32, verbose=True):\n",
    "        fig = plt.figure()\n",
    "        nOfBatches = int(np.ceil(X.shape[0] / batchSize))\n",
    "\n",
    "        total_train_cost_hist = np.zeros((epochs))\n",
    "        total_val_cost_hist = np.zeros((epochs))        \n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if verbose: print('Epoch:', e + 1)\n",
    "            total_batch_train_cost = 0\n",
    "            total_batch_val_cost = 0 \n",
    "            \n",
    "            for batchIndex in range(0, nOfBatches):\n",
    "                if verbose: print('\\r[{}/{}]: {}'.format(batchIndex + 1, nOfBatches, printProgressBar((batchIndex + 1)/nOfBatches)), end=\"\")\n",
    "\n",
    "                ### Get a batch from the training data\n",
    "                startAt = batchIndex*batchSize\n",
    "                endAt = startAt + batchSize\n",
    "                batch_X = np.transpose(X[startAt:endAt])\n",
    "                batch_y = np.transpose(y[startAt:endAt])\n",
    "                \n",
    "                ### Inititalize global parameters\n",
    "                n = batch_X.shape[1]\n",
    "                learningScalar = eta / n\n",
    "                DW = {}\n",
    "                DB = {}\n",
    "\n",
    "                ### Forward propagate and get cost\n",
    "                A_L = self.forwardProp(batch_X)\n",
    "                C = self.calculateCost(A_L, batch_y)\n",
    "                \n",
    "                total_batch_train_cost += np.abs(np.sum(C))\n",
    "                if type(val_X) != type(None) and type(val_y) != type(None):\n",
    "                    batch_val_X = np.transpose(val_X[startAt:endAt])\n",
    "                    batch_val_y = np.transpose(val_y[startAt:endAt])\n",
    "                    \n",
    "                    A_L_val = self.forwardProp(batch_val_X, dry=True)\n",
    "                    C_val = self.calculateCost(A_L_val, batch_val_y)\n",
    "                    total_batch_val_cost += np.abs(np.sum(C_val))\n",
    "                \n",
    "                dL = C * self.output_layer.activationDerivative(self.output_layer.Z)\n",
    "                \n",
    "                ### Assign parameter update matricies for last layer\n",
    "                DW[self.size - 1] = learningScalar * np.dot(dL, np.transpose(self.output_layer.A_in))\n",
    "                DB[self.size - 1] = learningScalar * dL\n",
    "\n",
    "                ### Backpropagate and assign parameter update matricies for all other layers\n",
    "                self.backwardProp(dL, learningScalar, DW, DB)\n",
    "\n",
    "                ### Update all model parameters\n",
    "                self.updateParameters(DW, DB)\n",
    "\n",
    "            if verbose: print('\\n')\n",
    "            total_train_cost_hist[e] = total_batch_train_cost / nOfBatches\n",
    "            total_val_cost_hist[e] = total_batch_val_cost / nOfBatches\n",
    "            \n",
    "        return total_train_cost_hist, total_val_cost_hist\n",
    "    \n",
    "    def calculateCost(self, A_L, y):\n",
    "        C = A_L - y\n",
    "        return C\n",
    "        \n",
    "    def updateParameters(self, DW, DB):\n",
    "        for i in range(0,self.size):\n",
    "            layer = self.layers[i]\n",
    "            layer.W -= DW[i]\n",
    "            layer.b -= np.sum(DB[i], axis=1).reshape(layer.b.shape[0], 1)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if not (self.built):\n",
    "            raise Error('Model has not been built yet')\n",
    "        return self.forwardProp(X, dry=True)\n",
    "    \n",
    "    def printLayers(self):\n",
    "        print('Layer 0: ({},)'.format(self.input_shape))\n",
    "        i = 1\n",
    "        for layer in self.layers:\n",
    "            print()\n",
    "            print('Layer {}: {} {}'.format(i, layer.units, 'neuron' if layer.units == 1 else 'neurons'))\n",
    "            print('W dims = {}'.format(layer.W.shape))\n",
    "            print('b dims = {}'.format(layer.b.shape))\n",
    "            i += 1\n",
    "            print()\n",
    "\n",
    "    def __str__(self):\n",
    "        print('\\nSequential\\n')\n",
    "        for layer in self.layers:\n",
    "            if layer.built:\n",
    "                print('W shape:', layer.get_weights()[0].shape)\n",
    "                print('b shape:', layer.get_weights()[1].shape)\n",
    "                print('\\n')\n",
    "        \n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a560437",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='linear')\n",
    "])\n",
    "\n",
    "model.build(train_X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b13fc",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e989373",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "train_cost_hist, val_cost_hist = model.fit(train_X, train_y, val_X, val_y, epochs=epochs, batchSize=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae117c",
   "metadata": {},
   "source": [
    "## Visulize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ae86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_cost_hist, 'r-')\n",
    "plt.plot(range(epochs), val_cost_hist, 'g-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1baa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_X = np.transpose(test_X)\n",
    "actual_y = np.transpose(test_y)\n",
    "predictions = model.predict(actual_X)\n",
    "\n",
    "total_examples = actual_y.shape[1]\n",
    "total_correct = 0\n",
    "for i in range(0, total_examples):\n",
    "    predictedDigit = np.argmax(tf.nn.softmax(predictions[:, i]))\n",
    "    actualDigit = np.argmax(actual_y[:, i])\n",
    "    if predictedDigit == actualDigit:\n",
    "        total_correct += 1\n",
    "\n",
    "print('Accuracy: {}'.format(total_correct / total_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e8a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b0e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
